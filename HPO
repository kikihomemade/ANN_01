import numpy as np
import pandas as pd
#import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt



#Call data(csvv-file) "/Users/muqiliu/Desktop/CT.csv"
dataset = pd.read_csv("/Users/muqiliu/Desktop/CT.csv", header = None)#, verbose=False, sep=";"
dataset = dataset.drop(0)

X = dataset.iloc[:,1:6] #unabhängige Variable
y = dataset.iloc[:,6] #abhängige Variable

X=np.asarray(X).astype(np.float32)
y=np.asarray(y).astype(np.float32).reshape(-1, 1)

#Normalizing the data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
y = sc.fit_transform(y)


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)

############################## in sklearn model umwandeln ###############################
 
from keras.layers import Dense,Activation,Embedding,Flatten,LeakyReLU,BatchNormalization
from keras.activations import relu,sigmoid


def build_model(layer_size = 30,
                learning_rate = 3e-3,
                activation = 'sigmoid' ):  
    
  model=keras.Sequential()
  model.add(layers.Dense(layer_size,activation))
  model.add(layers.Dense(1, activation))
      
  model.compile(optimizer='adam',loss='mean_squared_error')
  return model

sklearn_model = keras.wrappers.scikit_learn.KerasRegressor(build_fn = build_model)
#callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)]
history = sklearn_model.fit(X_train, y_train,
                            epochs = 100,
                            validation_data = (X_test, y_test))

# define parameter set & Search parameters
param_distribution = {
    "activation": ['sigmoid','relu'],
    "layer_size": np.arange(1, 100),
    "learning_rate": reciprocal(1e-4, 1e-2)
}


######################  RandomizedSearch  #######################
from sklearn.model_selection import RandomizedSearchCV
random_search_cv = RandomizedSearchCV(sklearn_model,
                                     param_distribution,
                                     n_iter = 10,
                                     cv = 3,
                                     n_jobs = 1)
random_search_cv.fit(X_train, y_train, epochs = 100,
                    validation_data = (X_test, y_test))

# cross_validation: 训练集分成n份，n-1训练，最后一份验证.
print(random_search_cv.best_params_)
print(random_search_cv.best_score_)
print(random_search_cv.best_estimator_)


######################  GridSearchCV  ###########################
from sklearn.model_selection import  GridSearchCV
import xgboost as xgb
param_grids = {
    "activation": ['sigmoid','relu'],
    "layer_size": [32, 64, 128, 256, 512],
    "learning_rate": [0.001,0.01,0.1]

}

#clf1 = xgb.XGBClassifier()
grid = GridSearchCV(estimator=model,
        param_grid=param_grids, 
        cv=5, 
        n_jobs=-1, 
        scoring='neg_mean_squared_error',
        verbose=2)
grid_result = grid.fit(X_train, y_train)
#best_estimator = grid.best_estimator_

print(grid_result.best_params_)
print(grid_result.best_score_)




    
model = random_search_cv.best_estimator_.model
model.evaluate(X_test, y_test)
