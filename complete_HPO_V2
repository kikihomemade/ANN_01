#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Apr 15 10:47:55 2022

@author: muqiliu
"""

import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error#Import the evaluation index of regression model - mean square error MSE, fitting Youdu R2, mean absolute error
import sherpa
import tensorflow
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
import tempfile
import os
import shutil
import tensorflow.keras.backend as K
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import cm
import time
import numpy as np

OPTIMIZATION_LOSS=100 #optimization iteration
COMPARE_LOSS=1000     #contrast iteration
#=======================================================================
#Defining Neural Network Functions
#=======================================================================
def build_model(layer_size,
                learning_rate,
                activation ):  
    
  model=keras.Sequential()
  model.add(layers.Dense(layer_size,activation))
  model.add(layers.Dense(1, "linear"))
  model.compile(optimizer = Adam(lr=learning_rate),loss='mean_squared_error')
  return model      
#=======================================================================
#Data processing and analysis
#=======================================================================
#Call data(csvv-file) "/Users/muqiliu/Desktop/CT.csv"
dataset = pd.read_csv("CT.csv", header = None)#, verbose=False, sep=";"
dataset = dataset.drop(0)

X = dataset.iloc[:,1:6] #unabhängige Variable
y = dataset.iloc[:,6] #abhängige Variable

X=np.asarray(X).astype(np.float32)
y=np.asarray(y).astype(np.float32).reshape(-1, 1)



#Normalizing the data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
y = sc.fit_transform(y)


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

#=======================================================================
#PBT
#=======================================================================
parameters = [sherpa.Continuous('learning_rate', [1e-4, 1e-2]),
              sherpa.Continuous('layer_size', [3, 70]),
             sherpa.Choice("activation", ['sigmoid','relu','tanh'])]
algorithm = sherpa.algorithms.PopulationBasedTraining(population_size=5,
                                                      num_generations=5,
                                                      perturbation_factors=(0.8, 1.2),
                                                      parameter_range={'learning_rate': [1e-6, 1e-1]})
study = sherpa.Study(parameters=parameters,
                     algorithm=algorithm,
                     lower_is_better=True,
                     disable_dashboard=True)
startTime=time.time()
for trial in study:
    generation =trial.parameters['generation']
    training_lr =trial.parameters['learning_rate']
    trial.parameters['layer_size']=int(trial.parameters['layer_size'])
    training_layer_size=trial.parameters['layer_size']
    training_activation=trial.parameters['activation']

    print("-"*100)
    print("Generation {}".format(generation))    

    print("Creating new model with learning rate {}\n".format(training_lr))

    # Create model
  
    # Use learning rate parameter for optimizer  
    model=build_model(layer_size = training_layer_size,
                    learning_rate = training_lr,
                    activation = training_activation)
    # Train model for one epoch   
    model.fit(X_train, y_train,epochs = OPTIMIZATION_LOSS, batch_size = 45,verbose=0)
    loss = model.evaluate(X_test, y_test)    
    print("Validation loss: ", loss)
    study.add_observation(trial, iteration=generation,objective=loss,context={'training_error': loss})
    study.finalize(trial=trial)
    
endTime=time.time()     
PBT_best_result=study.get_best_result()

#=======================================================================
#RandomizedSearch
#=======================================================================
sklearn_model = keras.wrappers.scikit_learn.KerasRegressor(build_fn = build_model)
from sklearn.model_selection import RandomizedSearchCV# define parameter set & Search parameters
from scipy.stats import reciprocal
param_distribution = {
    "activation": ['sigmoid','relu','tanh'],
    "layer_size": np.arange(3, 70),
    "learning_rate": reciprocal(1e-4, 1e-2)
}
startTime_RSCV=time.time()

random_search_cv = RandomizedSearchCV(sklearn_model,
                                     param_distribution,
                                     n_iter = 2,
                                     cv = 3,
                                     n_jobs = 1)
random_search_cv.fit(X_train, y_train, epochs = OPTIMIZATION_LOSS,
                    validation_data = (X_test, y_test))
endTime_RSCV=time.time()
# cross_validation: The training set is divided into n parts, n-1 for training, and the last for validation.
RSCV_best_result=random_search_cv.best_params_


#=======================================================================
#GridSearchCV
#=======================================================================
from sklearn.model_selection import  GridSearchCV
#import xgboost as xgb


param_grids = {
    "activation": ['sigmoid','relu','tanh'],
    "layer_size": [4, 8, 16, 32, 64],
    "learning_rate": np.arange(1e-4, 1e-2)#[0.001,0.01,0.1]

}
startTime_grid=time.time()
#clf1 = xgb.XGBClassifier()
grid = GridSearchCV(estimator=sklearn_model,
        param_grid=param_grids, 
        cv=5, 
        n_jobs=-1, 
        scoring='neg_mean_squared_error',
        verbose=2)
grid_result = grid.fit(X_train, y_train,epochs = OPTIMIZATION_LOSS)
#best_estimator = grid.best_estimator_
endTime_grid=time.time()
print(grid_result.best_params_)
print(grid_result.best_score_)
grid_best_result=grid_result.best_params_



#=======================================================================
#Hyperopt 
#=======================================================================
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
#from sklearn.model_selection import cross_val_score
import sys
#'activation':hp.pchoice('activation', [(0.5,'relu'),(0.5,'sigmoid')]),
space = {'activation':hp.choice('activation',['relu','sigmoid','tanh']),
    "learning_rate": hp.uniform('learning_rate',  0.0001, 0.01),
    "layer_size": hp.uniformint('layer_size', 3,70),
}

def f_nn(params):  
  model=keras.Sequential()
  model.add(layers.Dense(params['layer_size'],params['activation']))
  model.add(layers.Dense(1, 'linear'))
      
  model.compile(optimizer = Adam(lr=params['learning_rate']),loss='mean_squared_error')
  model.fit(X_train, y_train,epochs = OPTIMIZATION_LOSS, batch_size = 45,verbose=0,
                              validation_data = (X_test, y_test))

  #pred_auc =model.predict(X_test, batch_size = 128, verbose = 0)
  #acc = roc_auc_score(y_test, pred_auc)
  #acc = model.evaluate(X_test, y_test)
  acc = model.evaluate(X_test, y_test, verbose=0)

  print('AUC:', acc)
  sys.stdout.flush() 
  return {'loss': -acc, 'status': STATUS_OK}  # Because fmin() tries to minimize the objective, this function must return the negative accuracy. 

startTime_hyper=time.time()
# Instantiate the Trial object, fine-tune the model, and print the best loss with its hyperparameter values
trials = Trials()
best = fmin(f_nn, space, algo=tpe.suggest, max_evals=6, trials=trials)
print("Best: {}".format(best))
hyper_best_result=best
if hyper_best_result['activation']==0:
    hyper_best_result['activation']='relu'
if hyper_best_result['activation']==1:
    hyper_best_result['activation']='sigmoid'
if hyper_best_result['activation']==2:
    hyper_best_result['activation']='tanh'    
        
endTime_hyper=time.time()

#=======================================================================
#result output
#=======================================================================
model=build_model(layer_size = PBT_best_result['layer_size'],
                learning_rate = PBT_best_result['learning_rate'],
                activation = PBT_best_result['activation'])
# Train model for one epoch   
history=model.fit(X_train, y_train,epochs = COMPARE_LOSS, batch_size = 45,verbose=1)
history_PBT=pd.DataFrame(history.history)

loss_PBT = model.evaluate(X_test, y_test)

model=build_model(layer_size = RSCV_best_result['layer_size'],
                learning_rate = RSCV_best_result['learning_rate'],
                activation = RSCV_best_result['activation'])
# Train model for one epoch   
history=model.fit(X_train, y_train,epochs = COMPARE_LOSS, batch_size = 45,verbose=1)
history_RSCV=pd.DataFrame(history.history)
loss_RSCV=model.evaluate(X_test, y_test)

model=build_model(layer_size = grid_best_result['layer_size'],
                learning_rate = grid_best_result['learning_rate'],
                activation = grid_best_result['activation'])
# Train model for one epoch   
history=model.fit(X_train, y_train,epochs = COMPARE_LOSS, batch_size = 45,verbose=1)
history_grid=pd.DataFrame(history.history)
loss_grid=model.evaluate(X_test, y_test)

model=build_model(layer_size = hyper_best_result['layer_size'],
                learning_rate = hyper_best_result['learning_rate'],
                activation = hyper_best_result['activation']
                )
# Train model for one epoch   
history=model.fit(X_train, y_train,epochs = COMPARE_LOSS, batch_size = 45,verbose=1)
history_hyper=pd.DataFrame(history.history)
loss_hyper=model.evaluate(X_test, y_test)

lossResult=history_PBT
lossResult.columns=['PBT_loss']
optimizationResult=pd.DataFrame({"activation_optimization":[PBT_best_result['activation']]
                                 ,"learning_rate_optimization":[PBT_best_result['learning_rate']]
                                 ,"layer_size_optimization":[PBT_best_result['layer_size']]
                                 ,"Loss_optimization":[PBT_best_result['Objective']]
                                 ,"runTimes_optimization":[endTime-startTime]})
lossResult['RSCV_loss']=history_RSCV
new=pd.DataFrame({"activation_optimization":[RSCV_best_result['activation']]
                                 ,"learning_rate_optimization":[RSCV_best_result['learning_rate']]
                                 ,"layer_size_optimization":[RSCV_best_result['layer_size']]
                                 ,"Loss_optimization":[loss_RSCV]
                                 ,"runTimes_optimization":[endTime_RSCV-startTime_RSCV]})
optimizationResult=optimizationResult.append(new)
lossResult['grid_loss']=history_grid
new=pd.DataFrame({"activation_optimization":[grid_best_result['activation']]
                                 ,"learning_rate_optimization":[grid_best_result['learning_rate']]
                                 ,"layer_size_optimization":[grid_best_result['layer_size']]
                                 ,"Loss_optimization":[loss_grid]
                                 ,"runTimes_optimization":[endTime_grid-startTime_grid]})
optimizationResult=optimizationResult.append(new)

lossResult['hyper_loss']=history_hyper
new=pd.DataFrame({"activation_optimization":[hyper_best_result['activation']]
                                 ,"learning_rate_optimization":[hyper_best_result['learning_rate']]
                                 ,"layer_size_optimization":[hyper_best_result['layer_size']]
                                 ,"Loss_optimization":[loss_hyper]
                                 ,"runTimes_optimization":[endTime_hyper-startTime_hyper]})
optimizationResult=optimizationResult.append(new)


optimizationResult.index = ['PBT','RSCV','grid','hyper']

lossResult.to_excel("lossResult.xlsx")

optimizationResult.to_excel("optimizationResult.xlsx")

#=======================================================================
#Contrast display
#=======================================================================

plt.plot(lossResult,label=['PBT','RSCV','grid','hyper'])

plt.legend()

plt.xlabel('iterations')

plt.ylabel('Loss')

plt.show()
